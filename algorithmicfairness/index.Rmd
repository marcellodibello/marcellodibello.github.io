---
title: "Algorithmic Fairness"
subtitle: ""
author: "Marcello Di Bello  marcello.dibello@lehman.cuny.edu"
output:
  rmdformats::html_clean:
    fig_width: 6
    fig_height: 6
    highlight: kate
    thumbnails: true
    lightbox: true
    gallery: true
    toc_depth: 2
    toc: TRUE
---    




<!-- 
OTHER INITIAL SETTING 

---
title: "Algorithmic Fairness"
author: "Marcello Di Bello"
output:
  rmdformats::html_clean:
    fig_width: 6
    fig_height: 6
    highlight: kate
    thumbnails: true
    lightbox: true
    gallery: true
    toc_depth: 2
    toc: TRUE
---    
    
--->


<!-- 
OTHER INITIAL SETTING PRETTY HTML

---
pagetitle: "Algorithmic Fairness"
title: "Algorithmic Fairness"
subtitle: ""
author: "Marcello Di Bello"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    navbar: yes
    toc: yes
    toc_depth: 3
    toc_float: no
---
-->



<!-- 
---
title: "Algorithmic Fairness"
author: "Marcello Di Bello"
output:
  html_document:
    toc: true
    navbar: yes
    toc_depth: 3
    toc_float: yes
    theme: cosmo
  pdf_document:
    toc: false
  highlight: zenburn    
---
--->



<script>
   $(document).ready(function() {
     $head = $('#header');
     $head.prepend('<img src=\"algo-fair-logo.png\" style=\"float: right;width: 350px;height: 220px;\"/>')
   });
</script>


<!---
<img src="data-ethics-logo.jpg" style="position:absolute;top:0px;right:0px;" />
--->

```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(knitr)
library(rmdformats)
## Global options
options(max.print = "75")
opts_chunk$set(echo = FALSE,
	             cache = FALSE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
      font-family:'Avenir Next';
  }

</style>



<!--- 
ADDED STYLES/ FONT SIZSES


<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }
  
 
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}

</style>
-->



# Course description

Public and private sector entities rely on algorithms to streamline decisions about 
healthcare, child abuse, public housing, bail and sentencing. 
This course examines the ongoing debate among computer scientists, economists, 
legal scholars and moral philosophers 
about the fairness of algorithmic decision-making.  What does it mean for algorithmic decisions to be fair? 
How does algorithmic fairness relate to other notions such as equal treatment, anti-discrimination and fair equality 
of opportunity? What are the formal tools for measuring fairness? What other values and goals 
should inform the design of ethical algorithms, such as minimizing the harm toward disavatanged minorities or respecting 
each person's individual circumstances?



# Objectives

This course is meant for advanced undergraduates or graduates students. 
Students will become familiar with different conceptions of algorithmic fairness,
such as predictive parity, classification parity and counterfactual fairness, as well as 
explore connections between algorithmic
fairness, anti-discrimination law and philosophical concepts such as fair equality of opportunity. 
Students will also develop critical thinking skills, in reading  and  writing, 
for the analysis of new technologies and the ethical issues they pose. Students will 
read academic papers in different disciplines---computer science, economics, law 
and philosohy---and will develop an appreciation for differences in scholarship 
across disciplines. 

# Similar courses

- Aaron Fraenkel's [Fairness and Algorithmic Decision Making](https://afraenkel.github.io/portfolio/curric-3-fairness/) - UC, San Diego

- Solon Barocas, Moritz Hardt, Arvind Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) -- book

- Moritz Hardt's [Fairness in Machine Learning](https://fairmlclass.github.io/) - UC, Berkeley

- Arvind Narayanan's [Fairness in Machine Learning](https://docs.google.com/document/d/1XnbJXELA0L3CX41MxySdPsZ-HNECxPtAw4-kZRc7OPI/edit) - Princeton



# Schedule & Readings


## Week 1: ProPublica v. Northpointe

Data-based predictive and scoring 
algorithms are now everywhere. They are used by private and public sector entities: banks, social workers,
police, judges, etc. While algorithms are efficient and usable in large scale, are they 
going to treat people fairly? 

ProPublica argued that [COMPAS](https://en.wikipedia.org/wiki/COMPAS_(software)) -- a predictive algorithm used in criminal justice to help judges make decisions about bail, preventative detention and sentencing -- 
is biased against black defendants. 

- ProPublica, [Machine Bias: There's software used across the country to predict future criminals. And it’s biased against blacks.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

Northpointe, the company that designed COMPAS, rejected the accusation.

- William Dieterich, Christina Mendoza, and Tim Brennan. [COMPAS risk scales: Demonstrating accuracy equity and predictive parity performance of the COMPAS risk scales in Broward county](https://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf) 

ProPublica responded.

- ProPublica, [How We Analyzed the COMPAS Recidivism Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

Others joined the debate.

- Anthony W. Flores, Kristin Bechtel, and Christopher T. Lowenkamp. [False positives, false negatives, and false analyses: A rejoinder to “Machine bias”](https://www.uscourts.gov/federal-probation-journal/2016/09/false-positives-false-negatives-and-false-analyses-rejoinder)

- [Technical Flaws of Pretrial Risk Assessments Raise Grave Concerns](https://cyber.harvard.edu/story/2019-07/technical-flaws-pretrial-risk-assessments-raise-grave-concerns)





## Week 2: Guided by data 

Let's take a step back and examine the basis of algorithmic decision-making. 
Data are abundant today. Though incomplete and never truly conclusive, data 
can guide our decisions, often saving lives. For example, data are playing a key role in the response to 
the COVID-19 pandemic. 

- Max Roser, Hannah Ritchie and Esteban Ortiz-Ospina, 
[Coronavirus Disease (COVID-19) - Statistics and Research](https://ourworldindata.org/coronavirus)

- Tomas Pueyo, [Coronavirus: Why You Must Act Now](https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca)

- Left, [Covid_19: Open letter from Italy to the international scientific community](https://left.it/2020/03/13/covid_19-open-letter-from-italy-to-the-international-scientific-community/?fbclid=IwAR0YbnRWpw0rzCFIb289bB74u6NkLs6tQeZN7JP4wAjv3wgWdOvB-chk7g4)


Another example is the data-driven Public Safety Assessment ([PSA](https://www.psapretrial.org/)) algorithm, 
now widely used in several jurisdictions in the United States to help judges make pre-trial decisions. According to a [report](https://njcourts.gov/courts/criminal/reform.html) from New Jersey, pre-trial jail population descrease significantly 
after PSA was adopted. The ACLU of New Jersey [endorsed](https://www.aclu-nj.org/theissues/criminaljustice/pretrial-justice-reform) 
the use of the PSA algorithm because it has the potential to end the pre-trial system of money and bail 
that disproportionately harms the poor.



## Week 3: Misguided by data

Although data can be helpful to make 
decisions, they can also be biased, partial or incomplete. 
Biases in the data often disdavatange racial minorities.

- *Joy Buolamwini, [Algorithms Aren’t Racist. Your Skin Is just too Dark](https://hackernoon.com/algorithms-arent-racist-your-skin-is-just-too-dark-4ed31a7304b8)

- *Moritz Hardt, [How Big Data Is Unfair](https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de)

Biases and gaps in the data are 
also often detrimental to women.

- Caroline C. Perez, [Invisible Women: Data Bias in a World Designed for Men](https://www.amazon.com/Invisible-Women-Data-World-Designed/dp/1419729071) -- read two chaptersyou find interesting 

Another problem is that, via feedback loops, decisions based on biased 
data magnify errors on a large scale especially when they are
automated. Garbage in, garbage *everywhere*.

- Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkata. [Runaway feedback loops in predictive policing](https://arxiv.org/abs/1706.09847) -- [video](https://www.youtube.com/watch?v=Qj_t8DL5TeQ)


## Week 4: The meanings of fairness 

What does it mean for algorithmic decisions to be fair in the first place? Computer scientists have formulated 
several different metrics of algorithmic fairness. By one count, there are as many as 21 definitions.

- Arvind Narayanan, [21 fairness definitions and their politics](https://www.youtube.com/watch?v=jIXIuYdnyyk)

- Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. [Fairness in criminal justice risk assessments: The state of the art](https://doi.org/10.1177/0049124118782533) 

Many of the definitions  of algorithmic 
fairness describe the extent to which *actual* rates of error are unevenly (and thus unfairly) allocated 
across different socially relevant groups. Scholars have also used
causal modeling to formulate a *counterfactual* definition of algorithmic fairness.

- Solon Barocas, Moritz Hardt, Arvind Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) -- chapter 2 and 4

- Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva, [Counterfactual Fairness](https://arxiv.org/abs/1703.06856)


## Week 5: Impossibility results 

It turns out that it is mathematically impossible,  
under  realistic conditions,  for algorithms
to satisfy different conceptions of fairness at the same time.

- Alexandra Chouldechova, [Fair Prediction with Disparate Impact](https://arxiv.org/pdf/1703.00056.pdf)

- Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan, [Inherent Trade-Offs in the Fair Determination of Risk Scores](https://arxiv.org/abs/1609.05807) -- [video](https://www.youtube.com/watch?v=K7i_tnflZ64) 

These impossibility results have led scholars to consider trade-offs 
between different conceptions of fairness or 
be skeptical toward existing definitions.

- Deborah Hellman, [Measuring Algorithmic Fairness](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3418528)

- Sam Corbett-Davies and Sharad Goel. [The measure and mismeasure of fairness: A critical review of fair machine learning](https://arxiv.org/abs/1808.00023)

**First paper due**


## Week 6: Testing for discrimination 

The academic debate about algorithmic fairness mirrors, in some important ways, 
the debate among economists about different tests for what counts as 
discriminatory behaviour. 


- Gary Becker, [The Economic way of Looking at Life](https://www.nobelprize.org/uploads/2018/06/becker-lecture.pdf)

- John Knowles, Nicola Persico, Petra Todd, [Racial Bias in Motor Vehicle Searches: Theory and Evidence](https://www.nber.org/papers/w7449)

- Solon Barocas, Moritz Hardt, Arvind Narayanan, [Fairness in Machine Learning](https://fairmlbook.org/) -- chapter 5

The problem of infra marginality helps to 
better understand the logic behind 
the impossibility results in algorithmic fairness.

- Camelia Simoiu, Sam Corbett-Davies and Sharad Goel, [The Problem of Infra-Marginality](https://5harad.com/papers/threshold-test.pdf)


## Week 7: Equal protection jurisprudence

Algorithimic fairneess is closely conected with antidiscrimination 
and equal protection jurisprudence, specifically the notions 
of "disparate treatment" and "disparate impact". 

Any differential treatment that is due to racial animus or discriminatory intent is clearly illegal. The use of race as a factor is not automatically prohibited, but must be justified with a clear rationale, an inquiry known as “strict scrutiny”. The Supreme Court has allowed the use of race in specified contexts, for example, in college admissions for the purpose of fostering diversity.

- [Fisher v. University of Texas (2016)](https://www.oyez.org/cases/2015/14-981)

Evidence of disparate impact against a protected group is enough to make a prima facie case of discrimination. This applies to sectors such as employment and housing. 

- [Title VII of the Civil Rights Act of 1964](https://www.eeoc.gov/laws/statutes/titlevii.cfm)

- [Hazelwood School District v. United States (1977)](https://en.wikipedia.org/wiki/Hazelwood_School_District_v._United_States) 

The criminal justice system, however, seems exempt. The US Supreme Court ruled that disparate racial impact is not enough to establish a constitutional violation. An elaborate statistical analysis – showing that death penalty decisions in Georgia disproportionately targeted African Americans, controlling for several variables – was not enough to convince the 
Court that the system violated equal protection.

- [In McClensky v. Kemp (1987)](https://www.oyez.org/cases/1986/84-6811) 

It is worth thinking about whether the legal notions of disparate treatment and disparate impact 
can help to understand what algorithmic fairness consists in. 

## Week 8: Fairness v. welfare

Utilitarianism, roughly, 
says that the right action is one that maximizes social welfare overall. 

- Jeremy Bentham, "Introduction to the Principles of Morals and Legislation", 
[Ch. I–II](https://www.econlib.org/library/Bentham/bnthPML.html)


If utilitarianism is the correct 
ethical theory, algorithms should maximize 
overall social welfare and this may 
require to treat people unfairly. 

- Louis Kaplow and Steven Shavell, [Fairness Versus Welfare](https://www.nber.org/papers/w9622)

- Aziz Z. Huq, [Racial Equity in Algorithmic Criminal Justice](https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=3972&context=dlj)



## Week 9: Equality 

Against utilitarianism, [John Rawls](https://plato.stanford.edu/entries/rawls/), 
an influential political philosopher of the 20th century, argued that 
society must give everyone equal opportunities to develop their talents and that 
inequalities in the distribution of goods can be tolerated only if they work 
to the advantage of the worst-off in society. What would algorithmic fairness 
look like in light of Rawls's view about equal opportunities and 
distributive justice?

- John Rawls, A Theory of Justice, 3, 11–12

Other influential philosophers, [Elizabeth Anderson](https://www.newyorker.com/magazine/2019/01/07/the-philosopher-redefining-equality), Tim Scanlon and Niko
Kolodny, have also defended  egalitarian views. 

- Elizabeth S. Anderson, [What is the Point of Equality?](https://www.philosophy.rutgers.edu/joomlatools-files/docman-files/4ElizabethAnderson.pdf)

- T. M. Scanlon, [Why Does Inequality Matter?](https://law.yale.edu/sites/default/files/documents/pdf/Intellectual_Life/ltw-Scanlon.pdf) 

- Niko Kolodny, [Why equality of treatment and opportunity might matter](https://philpapers.org/rec/KOLWEO)


What would algorithmic fairness 
look like in light of these egaliatarian views?


- Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. [A moral framework for understanding fair ML through economic models of equality of opportunity](https://www.cs.cornell.edu/~hh732/heidari2019moral.pdf)

- Reuben Binns, [Fairness in Machine Learning: Lessons from Political Philosophy](http://proceedings.mlr.press/v81/binns18a/binns18a.pdf)


## Week 10: Individualized decisions 

Since algorithms often rely on statistical generalizations and correlations, 
some worry that algorithms disregard  the individual circumstances 
of each person. Do people have a right to be treated as individuals? 
What would that mean?

- Kasper Lippert-Rasmussen, [We Are All Different](https://link.springer.com/article/10.1007%2Fs10892-010-9095-6)

- Benjamin Eidelson, [Treating People as Individuals](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2298429)

- Erin Beeghly, [Failing to Treat Persons as Individuals](https://quod.lib.umich.edu/e/ergo/12405314.0005.026/--failing-to-treat-persons-as-individuals?rgn=main;view=fulltext)


**Second paper due**


# Assignments

## Pass/fail assignments (*low stake*)

Every week please write a **one-page précis** of one of the papers assigned for that week. 
The précis should describe: (a) topic of the paper; 
(b) main thesis (or main theses, if there are more than one); 
(c) supporting arguments; (d) objections to these arguments, complications or 
difficulties that the author considers (if any).


## Papers (*high stake*)

There will be two main graded assignments 
for this course, roughly **15 pages** each. 
The **first** will cover Week 1 through 5, the 
**second**  Week 6 through 10.

**1**. Write an imaginary *philosophical dialogue* between two characters impersonating 
ProPublica and Northpointe. Describe very carefully (a) 
ProPublica's accusation that COMPASS is racially biased and Northpointe's response; (b) assess what further technical and philosophical questions about the nature of fairness must be addressed in order to settle the debate. 
Please always defend the claims made in the dialogue 
with careful and reasoned arguments. 

**2**. Write a *philosophical argumentative paper* that compares and contrasts the formal notions of algorithmic fairness (see Week 4 and 5) with either (i) the economic literature on metrics of discrimination (see Week 6), (ii) equal protection jurprisprudence (see Week 7), (iii) the philosophical literature on fairness and equality (See Week 8, 9 and 10). Please always defend the claims you make with careful and reasoned arguments. Check out these [guidelines](http://www.jimpryor.net/teaching/guidelines/writing.html) on how to write a philosophical argumentative essay.



